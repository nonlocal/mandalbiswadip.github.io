# Neural Machine Translation by Jointly Learning to Align and Translate

Review of the Attention paper.

1. TOC
{:toc}

## Goals

  1. what is attention
  2. Why attention
  3. Why is it better than LSTM
  4. Is attention the go-to architecture for seq2seq models
  5. What attention can't do
  6. Where, when, how
  7. What are "long sequences" in general? 10 elements? 
  8. Is attention really necessary for short sequences? 
  

## Introduction

## NMT or seq2seq in general

## Leaning to align and translate : attention

## Experiment

## Results

## Discussion
